/*
 * To change this license header, choose License Headers in Project Properties.
 * To change this template file, choose Tools | Templates
 * and open the template in the editor.
 */
package nueral.network;

import java.io.IOException;
import java.util.Random;
import static nueral.network.Network.feedForward;

/**
 *
 * @author elicowa
 */
public class NueralNetwork {
    public static int batchSize = 2;
    public static int sizes[] = {784, 100, 10, 10};
    public static float lrate = (float) 0.03;
    public static int num_layers = sizes.length;
    public static float[][][] weights;
    public static float[][] batchLabelsTemplate;
    public static float[][][] RealOutputsTemplate;
    public static float[][][][] weightsTemplate;
    /**
     * @param args the command line arguments
     */
    public static void main(String[] args) {
        // TODO code application logic here
        try {
            MnistReader.readlabels();
            MnistReader.readimages();
        } catch (IOException e) {
            throw new RuntimeException("Input images or labels not found", e);
        }
        System.out.println("Image and label reading complete");
        Random r = new Random(); //getstartingweights
        weights = getStartingWeights(sizes);
        for (int i = 0; i < num_layers - 2; i++) { //-1 due to weight layers, -1 due to soft layer
            for (int k = 0; k < sizes[i + 1]; k++) {
                for (int j = 0; j < sizes[i]; j++) {
                    weights[i][k][j] = ((float) r.nextGaussian()) / 100;
                }
            }
        }
        batchLabelsTemplate = new float[batchSize][sizes[sizes.length - 1]]; //batchlabels is used to create an expected soft layer output
        for (int i = 0; i < batchSize; i++) {
            for (int k = 0; k < sizes[sizes.length - 1]; k++) {
                batchLabelsTemplate[i][k] = (float) 0.0;
            }
        }
        RealOutputsTemplate = new float[batchSize][][]; //initalize RealOutputsTemplate
        for (int i = 0; i < RealOutputsTemplate.length; i++) {
            RealOutputsTemplate[i] = new float[sizes.length - 1][]; //due to removal of softLayer
            for (int k = 0; k < RealOutputsTemplate[i].length; k++) {
                RealOutputsTemplate[i][k] = new float[sizes[k]];
            }
        }
        weightsTemplate = new float[batchSize][][][];
        for (int i = 0; i < weightsTemplate.length; i++) {
            weightsTemplate[i] = getStartingWeights(sizes);
        }
        double num = Math.random();
        while (true) {
            float[][] batch = new float[batchSize][];
            float[][] batchLabels = batchLabelsTemplate.clone();
            for (int l = 0; l < batchSize; l++) {
                batch[l] = MnistReader.data[(int) (num * MnistReader.labels.length)]; //TODO don't reuse images
                batchLabels[l][MnistReader.labels[(int) (num * MnistReader.labels.length)]] = (float) 1.0;
                System.out.println(MnistReader.labels[(int) (num * MnistReader.labels.length)]);
            }
            float[][][] outputs = feedForward(weights, batch, sizes);
            int correctClassifications = 0;
            for (int iter = 0; iter < outputs.length; iter++) {
                if (outputs[iter][outputs[iter].length - 1] == batchLabels[iter]) {
                    correctClassifications++;
                }
            }
            float[][][] realOutputs = new float[outputs.length][][]; //excluding soft layer
            for (int i = 0; i < realOutputs.length; i++) {
                realOutputs[i] = new float[outputs[i].length - 1][]; //ignores the last layer of outputs
                for (int k = 0; k < realOutputs[i].length; k++) {
                    realOutputs[i][k] = outputs[i][k];
                }
            }
            System.out.println(correctClassifications);
            weights = Backprop.backPropigate(realOutputs, batchLabels, weights);
        }
        //for (int m = 0; m < batchSize; m++) {
        //for (int l = 0; l < outputs[m].length; l++) {
        //System.out.print(outputs[m][l] + ",");
        //}
        //System.out.println(MnistReader.labels[m]);
        //}
    }
//MnistReader.getEpoche(batchSize);
//}
    public static void init(int[] weights) {
    }
    public static float[][][] getStartingWeights(int[] size) {
        float tempWeights[][][] = new float[num_layers - 2][][]; // layer x output from nueron
        for (int p = 0; p < tempWeights.length; p++) {
            tempWeights[p] = new float[sizes[p + 1]][sizes[p]];
        }
        return tempWeights;
    }
}
