/*
 * To change this license header, choose License Headers in Project Properties.
 * To change this template file, choose Tools | Templates
 * and open the template in the editor.
 */
package nueral.network;
//The problem: I am ony calculating the error for a single nueron rather than Etotal. I need calcualte how a weight effects a

import static nueral.network.NueralNetwork.outputsTemplate;
import static nueral.network.NueralNetwork.weightsTemplate;

/**
 *
 * @author elicowa
 */
public class Backprop {
    private static float[][][] derE_derOut; //keep track of the derivitive of the total error with respect to the Out of a nueron.
    private static float[][][] derOut_derNet; // keep track of the derivitive of the out with respect to the new of a nueron. "der of sigmoid"
    private static float[][][][] derE_derWeight; //track of the derivative of the total error with respect to a Weight. "The holy grail of backprop"
    private static float[][][] deltas; //derEROut * derOutRNet "derE_derNet"
    private static int batchSize;
    //propogation -forwardfeed
    public static float[][][] backPropigate(float[][][] outputs, float[][] desiredOutputs, float[][][] weights) {
        //outputs: the array of all outputs for each iteration. image X layer X nueron
        //desiredOutputs: the array of all desired outputs. Image index w/ respect to weight and outputs X expected output.
        //weights: the array of all weights used in the working batch. Layer X nueronTo X nueronFrom
        //netoutputs: the array of all net values to each nueron. Image X Layer X nueron
        batchSize = outputs.length;
        derE_derOut = outputsTemplate.clone();
        derOut_derNet = outputsTemplate.clone();
        derE_derWeight = weightsTemplate.clone();
        deltas = outputsTemplate.clone();
        for (int i = 0; i < batchSize; i++) {
            for (int k = 0; k < desiredOutputs[i].length; k++) {
                derE_derOut[i][outputs[i].length - 1][k] = derErrorROut(outputs[i][outputs[i].length - 1][k], desiredOutputs[i][k]);
            }
        }
        for (int iterNum = 0; iterNum < batchSize; iterNum++) { //iteration number. "which image is being backproped"
            for (int layNum = weights.length - 1; layNum > 0; layNum--) { //weight layer number. "which weight layer is being proped"/ This part is tricky becuase the 0th layer of nueron connects to the 0th layer weight. So there can be a nth layer nuron but not an nth layer weight in the output layer.
                for (int nueNum = 0; nueNum < weights[layNum].length; nueNum++) { //nueron number. "which nueron's weights are being analysed"
                    //derE_derOut[iterNum][layNum+1][nueNum] = derErrorROut(outputs[iterNum][layNum+1][nueNum], targetOutputs[iterNum][layNum+1][nueNum]); //layNum+1 becuase layNum is based off a weight layer rather than a nueron layer (every network has n nueron layers and n-1 weight layers)
                    derOut_derNet[iterNum][layNum + 1][nueNum] = derOutRNet(outputs[iterNum][layNum + 1][nueNum]); //see comment above on layNum+1
                    deltas[iterNum][layNum + 1][nueNum] = derE_derOut[iterNum][layNum + 1][nueNum] * derOut_derNet[iterNum][layNum + 1][nueNum];
                    for (int weiNum = 0; weiNum < weights[layNum][nueNum].length; weiNum++) {
                        System.out.println(weiNum + " " + nueNum + " " + layNum);
                        derE_derWeight[iterNum][layNum + 1][nueNum][weiNum] = deltas[iterNum][layNum + 1][nueNum] * derNetRWei(outputs[iterNum][layNum][weiNum]);
                    }
                }
                for (int k = 0; k < outputs[iterNum][layNum].length; k++) { //num of nuerons in previous layer
                    float sum = 0;
                    for (int l = 0; l < outputs[iterNum][layNum + 1].length; l++) {
                        sum += deltas[iterNum][layNum + 1][l] * derFnet_RderPout(weights[layNum][l][k]); //derE/dernet * dernet/derpreviousout
                        derE_derOut[iterNum][layNum][k] = sum;
                    }
                }
            }
        }
        return updateWeights(derE_derWeight, weights);
    }
    public static float costFunc(int batchSize, float[][] outputs, float[] trueOutputs) { //calculation of cost
        //actual for each output unit - prediction
        float sum = 0;
        for (int i = 0; i < outputs[outputs.length - 2].length; i++) {
            sum += Math.pow((trueOutputs[i] - outputs[outputs.length - 2][i]), 2);
        }
        return sum * (1 / (2 * batchSize));
    }
    public static float derOutRNet(float out) { //der of sigmoid
        return out * (1 - out);
    }
    public static float derErrorROut(float out, float target) {
        return out - target;
    }
    public static float derNetRWei(float out) {
        return out;
    }
    public static float derFnet_RderPout(float weight) {
        return weight;
    }
    //weight update
    public static float[][][] updateWeights(float[][][][] derE_derWeight, float[][][] pWeights) { //previous Weights
        float[][][] nWeights = pWeights.clone(); //new weights itisalized as pWeights to maintain jagged array structure.
        for (int lay = 0; lay < pWeights.length; lay++) {
            for (int toN = 0; toN < pWeights[lay].length; toN++) {
                for (int fromN = 0; fromN < pWeights[lay][toN].length; fromN++) {
                    float avg = 0;
                    for (int iterN = 0; iterN < derE_derWeight.length; iterN++) {
                        avg += derE_derWeight[iterN][lay][toN][fromN];
                    }
                    avg = avg / derE_derWeight.length;
                    nWeights[lay][toN][fromN] = pWeights[lay][toN][fromN] - avg;
                }
            }
        }
        return nWeights;
    }
}
